{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Benchmark Titan Text Embeddings V2: Embeddings model on Amazon Bedrock\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are integral to various natural language processing applications, with their quality crucial for optimal performance. They are commonly used in knowledge bases to represent textual data as dense vectors enabling efficient similarity search and retrieval. In Retrieval Augmented Generation (RAG), embeddings are used to retrieve relevant passages from a corpus to provide context for language models to generate informed, knowledge-grounded responses. Embeddings also play a key role in personalization and recommendation systems by representing user preferences, item characteristics, and historical interactions as vectors, allowing calculation of similarities for personalized recommendations based on user behavior and item embeddings. As new embedding models are released with incremental quality improvements, organizations must weigh the potential benefits against the associated costs of upgrading, considering factors like computational resources, data preprocessing, integration efforts, and projected performance gains impacting business metrics.\n",
    "\n",
    "To learn more about Titan Text Embeddings V2, please refer to this link: [Titan Text Embeddings V2](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-models.html)\n",
    "\n",
    "#### How a piece of text is converted into a vector?\n",
    "Common approach is to use models which can provide contextualized embeddings for entire sentences. These models are based on deep learning architectures such as Transformers, which can capture the contextual information and relationships between words in a sentence more effectively.\n",
    "\n",
    "![Embedding Model](imgs/vector_embedding.png)\n",
    "\n",
    "In addition to semantic search, you can use embeddings to augment your prompts for more accurate results through Retrieval Augmented Generation (RAG)—but in order to use them, you’ll need to store them in a database with vector capabilities.\n",
    "\n",
    "![Embedding Model](imgs/vector_db.jpg)\n",
    "\n",
    "\n",
    "In September of 2023, Amazon announced the launch of Amazon Titan Text Embeddings V1, a multilingual text embeddings model that converts text inputs like single words, phrases, or large documents into high-dimensional numerical vector representations.  Since then 1000s of our customers used the first version of the model that supported over 25 languages, with an input up to 8,192 tokens, and outputs vector of 1,536 dimensions. Today we take that to next level by introducing a flexible output embedding model. The  Amazon Titan Text Embeddings V2 model supports over 100 languages and allows for a variable dimenion output thereby saving cost in terms of reduced size to store the embeddings. The output dimensions are 256, 384 and 1024. This model is designed to perform well on multi-lingual data and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Create a Bedrock client\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create a Bedrock client\n",
    "bedrock_client = boto3.client('bedrock')\n",
    "\n",
    "# Create a Bedrock Runtime client\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client.list_foundation_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the models\n",
    "all_models = [ model['modelId'] for model in bedrock_client.list_foundation_models()['modelSummaries'] ]\n",
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list only the Embedding models\n",
    "embedding_models = [model for model in all_models if 'embed' in model.lower()]\n",
    "embedding_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Generate Embeddings\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "prompt = \"Amazon Bedrock supports foundation models from industry-leading providers such as \\\n",
    "AI21 Labs, Anthropic, Stability AI, and Amazon. Choose the model that is best suited to achieving \\\n",
    "your unique goals.\"\n",
    "\n",
    "model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "body = json.dumps({\n",
    "    \"inputText\": prompt,\n",
    "    \"dimensions\": 256,\n",
    "    \"normalize\": False\n",
    "})\n",
    "\n",
    "model = bedrock_runtime_client.invoke_model(modelId=model_id, body=body, accept=\"application/json\", contentType=\"application/json\")\n",
    "\n",
    "response_body = json.loads(model.get('body').read())\n",
    "\n",
    "embedding = response_body.get(\"embedding\")\n",
    "print(f\"The embedding vector has {len(embedding)} values\\n{embedding[0:3]+['...']+embedding[-3:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magnitude of the vector\n",
    "np.linalg.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing you can use `amazon.titan-embed-text-v2` as embedding model via the API. The input text size is 8k tokens and the output vector length can be any of 256, 384 or 1024\n",
    "\n",
    "To use a text embeddings model, use the InvokeModel API operation or the Python SDK. Use InvokeModel to retrieve the vector representation of the input text from the specified model.\n",
    "Input\n",
    "\n",
    "```\n",
    "\n",
    "{\n",
    "    \"inputText\": text,\n",
    "    \"dimensions\": dimensions, # range from 256 , 384, 1024\n",
    "    \"normalize\": normalize\n",
    "}\n",
    "\n",
    "Output\n",
    "\n",
    "{\n",
    "    \"embedding\": []\n",
    "}\n",
    "```\n",
    "\n",
    "#### Normalization of a vector \n",
    "\n",
    "Normalization is the process of scaling it to have a unit length or magnitude of 1. It is useful to ensure that all vectors have the same scale and contribute equally during vector operations, preventing some vectors from dominating others due to their larger magnitudes.\n",
    "\n",
    "#### When should you Normalize:\n",
    "Use this as default for most of the use cases like Retrieval, RAG and others\n",
    "\n",
    "#### When you should not Normalize: \n",
    "Normnally normalization will work for all use cases, but you can experiment for certain use cases like Classification or Entity extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Amazon Bedrock supports foundation models from industry-leading providers such as \\\n",
    "AI21 Labs, Anthropic, Stability AI, and Amazon. Choose the model that is best suited to achieving \\\n",
    "your unique goals.\"\n",
    "\n",
    "model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "body = json.dumps({\n",
    "    \"inputText\": prompt,\n",
    "    \"dimensions\": 1024,\n",
    "    \"normalize\": True\n",
    "})\n",
    "\n",
    "model = bedrock_runtime_client.invoke_model(modelId=model_id, body=body, accept=\"application/json\", contentType=\"application/json\")\n",
    "\n",
    "response_body = json.loads(model.get('body').read())\n",
    "\n",
    "embedding = response_body.get(\"embedding\")\n",
    "print(f\"The embedding vector has {len(embedding)} values\\n{embedding[0:3]+['...']+embedding[-3:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# magnitude of the vector\n",
    "np.linalg.norm(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to get the embedding\n",
    "\n",
    "def get_embedding(prompt):\n",
    "    body = json.dumps({\n",
    "        \"inputText\": prompt,\n",
    "        \"dimensions\": 1024,\n",
    "        \"normalize\": True\n",
    "    })\n",
    "    model = bedrock_runtime_client.invoke_model(modelId=model_id, body=body, accept=\"application/json\", contentType=\"application/json\")\n",
    "    response_body = json.loads(model.get('body').read())\n",
    "\n",
    "    return response_body.get(\"embedding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the embedding with two different prompts and check the similarity\n",
    "\n",
    "text1 = \"Python is a programming language\"\n",
    "text2 = \"i have to go to the market\"\n",
    "\n",
    "text1_embedding = get_embedding(text1)\n",
    "text2_embedding = get_embedding(text2)\n",
    "\n",
    "# cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n",
    "cosine_similarity(text1_embedding, text2_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"Its a sunny day\"\n",
    "\n",
    "text3_embedding = get_embedding(text3)\n",
    "\n",
    "cosine_similarity(text1_embedding, text3_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 = \"Today it may not be raining\"\n",
    "\n",
    "text4_embedding = get_embedding(text4)\n",
    "\n",
    "cosine_similarity(text3_embedding, text4_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
