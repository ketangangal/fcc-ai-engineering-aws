{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade langchain-community\n",
    "# %pip install --upgrade langchain-aws \n",
    "# %pip install langchain langchain-text-splitters langchain-postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.16\n"
     ]
    }
   ],
   "source": [
    "import langchain_community\n",
    "\n",
    "print(langchain_community.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LLMs in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px;\">\n",
    "  <p>An alternative is to use the SDK of the LLM provider you started with, like <code>boto3</code> for AWS. However, learning LangChain offers clear benefits:</p>\n",
    "  \n",
    "  <ol>\n",
    "    <li><b>Ease of Use:</b> Simplifies working with LLMs by abstracting API complexities, reducing code.</li>\n",
    "    <li><b>Flexibility:</b> Supports multiple LLM providers, making it easy to switch services.</li>\n",
    "    <li><b>Integration:</b> Works well with popular libraries like PyTorch and TensorFlow.</li>\n",
    "    <li><b>Community:</b> Large, active community with frequent updates and support.</li>\n",
    "  </ol>\n",
    "\n",
    "  <h4>Prebuilt Patterns</h4>\n",
    "  <p>LangChain provides common LLM patterns (e.g., chain-of-thought) to help you quickly get started. Use these to see if they meet your needs before diving into more complex implementations.</p>\n",
    "\n",
    "  <h4>Interchangeable Components</h4>\n",
    "  <p>LangChain components (e.g., LLMs, output parsers) are easily swapped, future-proofing your application as models and needs evolve.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px;\">\n",
    "  <p><b>LangChain</b> offers two simple interfaces to interact with any LLM API provider:</p>\n",
    "  \n",
    "  <ul>\n",
    "    <li><b>LLMs</b></li>\n",
    "    <li><b>Chat models</b></li>\n",
    "  </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to be able to do more advanced tasks.\n",
      "Apologies, but the model is unable to browse the internet. It is a large language model (LLM) designed to assist users with their queries based on the information it was trained on.\n"
     ]
    }
   ],
   "source": [
    "# Using Bedrock LLM (Not Recommended Now)\n",
    "from langchain_aws import BedrockLLM\n",
    "\n",
    "llm = BedrockLLM(model_id=\"amazon.titan-text-express-v1\")\n",
    "\n",
    "prompt = \"I am learning LangChain now and going forward, \"\n",
    "completion = llm.invoke(prompt)\n",
    "print(completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's great! LangChain is a powerful and versatile framework for building applications with large language models (LLMs). Here are some tips and suggestions as you continue learning and working with LangChain:\n",
      "\n",
      "1. **Start with the basics**: If you're new to LangChain, begin by understanding the core concepts and components, such as agents, chains, prompts, and memory. The LangChain documentation and tutorials are excellent resources for getting started.\n",
      "\n",
      "2. **Explore the available tools and integrations**: LangChain provides a wide range of tools and integrations for various tasks, such as data ingestion, question answering, text generation, and more. Familiarize yourself with these tools and learn how to use them effectively.\n",
      "\n",
      "3. **Practice with examples and projects**: The best way to learn LangChain is by working on practical examples and projects. Start with simple use cases and gradually move towards more complex scenarios. The LangChain repository on GitHub has many examples that you can explore and modify.\n",
      "\n",
      "4. **Understand the limitations and best practices**: While LLMs are powerful, they have limitations and can produce biased or incorrect outputs. Learn about the best practices for using LLMs responsibly and mitigating potential risks.\n",
      "\n",
      "5. **Join the community**: LangChain has an active community of developers and researchers. Join the LangChain Discord server or follow the project on GitHub to stay up-to-date with the latest developments, ask questions, and share your experiences.\n",
      "\n",
      "6. **Experiment and iterate**: LangChain is a rapidly evolving framework, and new features and improvements are constantly being added. Don't be afraid to experiment with different approaches and iterate on your solutions as you gain more experience.\n",
      "\n",
      "7. **Consider contributing**: If you find bugs, have feature requests, or want to improve the documentation, consider contributing to the LangChain project. Open-source contributions are highly valued and can help you deepen your understanding of the framework.\n",
      "\n",
      "Remember, learning a new framework like LangChain takes time and practice. Be patient, stay curious, and enjoy the journey of building powerful applications with large language models.\n"
     ]
    }
   ],
   "source": [
    "# Using Chat models (Recommended)\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "bedrock_chat = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                           model_kwargs=dict(temperature=0, max_tokens=500))\n",
    "\n",
    "prompt = \"I am learning LangChain now and going forward, \"\n",
    "\n",
    "completion = bedrock_chat.invoke(prompt)\n",
    "print(completion.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px;\">\n",
    "  <h3 style=\"color:#2F4F4F;\"><b>Chat Model Interface</b></h3>\n",
    "  <p>Alternatively, the <b>Chat Model interface</b> enables conversations between the user and the model. Popular LLM providers like <b>Anthropic/Bedrock</b> classify messages into three roles:</p>\n",
    "  \n",
    "  <ul>\n",
    "    <li><b>System role:</b> Defines instructions the model should follow to answer questions.</li>\n",
    "    <li><b>User role:</b> Represents the individual asking questions and generating queries.</li>\n",
    "    <li><b>Assistant role:</b> The model's response to the user's query.</li>\n",
    "  </ul>\n",
    "  \n",
    "  <p>The <b>Chat models interface</b> simplifies configuration and management of conversations in AI chatbot applications.</p>\n",
    "  \n",
    "  <p>Here's an example using <b>LangChain’s ChatBedrock model</b> in Python:</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Barack Obama was the President of the United States in 2010. He served two terms from 2009 to 2017.', additional_kwargs={'usage': {'prompt_tokens': 20, 'completion_tokens': 31, 'total_tokens': 51}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 20, 'completion_tokens': 31, 'total_tokens': 51}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-969aa03f-5cc6-415d-bb08-c9ca4cbf4e10-0', usage_metadata={'input_tokens': 20, 'output_tokens': 31, 'total_tokens': 51})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "bedrock_chat = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                           model_kwargs=dict(temperature=0, max_tokens=500))\n",
    "\n",
    "prompt = [HumanMessage(content=\"Who was the president of the United States in 2010?\")]\n",
    "completion = bedrock_chat.invoke(prompt)\n",
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px;\">\n",
    "  <p>Unlike a single prompt string, <b>chat models</b> use different message types for each role:</p>\n",
    "  \n",
    "  <ul>\n",
    "    <li><b>HumanMessage:</b> From the human perspective (user role).</li>\n",
    "    <li><b>AIMessage:</b> From the AI perspective (assistant role).</li>\n",
    "    <li><b>SystemMessage:</b> Instructions for the AI (system role).</li>\n",
    "    <li><b>ChatMessage:</b> Allows arbitrary role setting.</li>\n",
    "  </ul>\n",
    "\n",
    "  <p>Let’s incorporate a <b>SystemMessage</b> instruction in our example, first in Python:</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Barack Obama was the president of the United States in 2010. He served two terms from 2009 to 2017. Why was the baseball player so successful? Because he had a good batting average!', additional_kwargs={'usage': {'prompt_tokens': 35, 'completion_tokens': 47, 'total_tokens': 82}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 35, 'completion_tokens': 47, 'total_tokens': 82}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-524bca02-8596-4df9-aa75-5368fcd1efc5-0', usage_metadata={'input_tokens': 35, 'output_tokens': 47, 'total_tokens': 82})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "bedrock_chat = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                           model_kwargs=dict(temperature=0, max_tokens=500))\n",
    "\n",
    "prompt = [\n",
    "            SystemMessage(content=\"You are a helpful assistant that answers questions with a joke at the end.\"),\n",
    "            HumanMessage(content=\"Who was the president of the United States in 2010?\")\n",
    "         ]\n",
    "completion = bedrock_chat.invoke(prompt)\n",
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making LLM prompts reusable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px;\">\n",
    "  <p><b>Here is an example of a detailed prompt:</b></p>\n",
    "\n",
    "  <p><b>Prompt:</b> Answer the question based on the context below. If the question cannot be answered using the information provided, respond with \"I don't know\".</p>\n",
    "\n",
    "  <p><b>Context:</b> The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers creating NLP applications. Developers can access these models through Hugging Face's <code>transformers</code> library, or by using the <code>bedrock</code> and <code>cohere</code> libraries.</p>\n",
    "\n",
    "  <p><b>Question:</b> Which model providers offer LLMs?</p>\n",
    "  \n",
    "  <p><b>Answer:</b></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, the model providers that offer Large Language Models (LLMs) are Hugging Face, Bedrock, and Cohere. The context mentions that developers can access LLMs through Hugging Face's `transformers` library, and Bedrock and Cohere's offerings through the `bedrock` and `cohere` libraries, respectively.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "# llm model \n",
    "model = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                    model_kwargs=dict(temperature=0, max_tokens=500))\n",
    "\n",
    "# both `template` and `model` can be reused many times\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don't know\".\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: \"\"\")\n",
    "\n",
    "# `prompt` and `completion` are the results of using template and model once\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing Bedrock and Cohere's offerings through the `bedrock` and `cohere` libraries respectively.\",\n",
    "    \n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "completion = model.invoke(prompt)\n",
    "\n",
    "print(completion.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px;\">\n",
    "  <p>Notice how the prompt contains instructions in a <b>SystemMessage</b> and two <b>HumanMessages</b> that contain dynamic <code>context</code> and <code>question</code> variables. You can format the template similarly and get a static prompt, which can then be passed to a large language model for a prediction output.</p>\n",
    "  \n",
    "  <p>Here’s how it looks first in Python:</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, the model providers that offer Large Language Models (LLMs) are:\n",
      "\n",
      "1. Hugging Face (through their `transformers` library)\n",
      "2. Bedrock\n",
      "3. Cohere (through their `cohere` library)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don\\'t know\".'),\n",
    "    ('human', 'Context: {context}'),\n",
    "    ('human', 'Question: {question}'),\n",
    "])\n",
    "model = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                    model_kwargs=dict(temperature=0, max_tokens=500))\n",
    "\n",
    "# `prompt` and `completion` are the results of using template and model once\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing Bedrock and Cohere's offerings through the `bedrock` and `cohere` libraries respectively.\",\n",
    "    \n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "\n",
    "completion = model.invoke(prompt)\n",
    "\n",
    "print(completion.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Specific Formats out of LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='A pound of bricks and a pound of feathers weigh the same.' justification='A pound is a unit of weight or mass, not a measure of volume or density. Since a pound of bricks and a pound of feathers both have the same mass (one pound), they must weigh the same amount. The fact that bricks are denser and more compact than feathers is irrelevant when the masses are equal. This is a classic example that illustrates the difference between weight and density.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user question along with justification for the answer.'''\n",
    "    answer: str  = Field(description=\"The answer to the user's question\")\n",
    "    justification: str  = Field(description=\"Justification for the answer\")\n",
    "\n",
    "llm = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                    model_kwargs=dict(temperature=0, max_tokens=500))\n",
    "\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "response = structured_llm.invoke(\"What weighs more, a pound of bricks or a pound of feathers\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px;\">\n",
    "  <h4><b>Imperative Composition</b></h4>\n",
    "  <p>Imperative composition is simply writing the code you're used to, composing LangChain components into functions and classes.</p>\n",
    "\n",
    "  <h4><b>Declarative Composition</b></h4>\n",
    "  <p><b>LangChain Expression Language (LCEL)</b> is a declarative language for composing LangChain components. LangChain compiles LCEL compositions into an optimized execution plan, offering features like automatic parallelization, streaming, tracing, and async support.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for Large Language Model. It refers to advanced natural language processing (NLP) models that are trained on vast amounts of text data to understand and generate human-like text. Some well-known examples of LLMs include GPT-3 (Generative Pre-trained Transformer 3) developed by OpenAI, BERT (Bidirectional Encoder Representations from Transformers) by Google, and LaMDA (Language Model for Dialogue Applications) by Google. LLMs can be used for various NLP tasks such as text generation, translation, summarization, question answering, and more. They have shown remarkable capabilities in understanding and producing human-like language, making them valuable tools in various applications and industries!!\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# the building blocks\n",
    "llm = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                    model_kwargs=dict(temperature=0, max_tokens=500))\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "                                                (\"system\", \"You are a helpful assistant, end the answer with ! 2 times.\"),\n",
    "                                                (\"human\", \"{question}\")\n",
    "                                            ])\n",
    "\n",
    "# combine them with the | operator\n",
    "chatbot = template | llm\n",
    "\n",
    "# inference\n",
    "completion = chatbot.invoke(\"What is LLM?\")\n",
    "\n",
    "print(completion.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LLM stands for Large Language Model. It refers to advanced natural language processing (NLP) models that are trained on vast amounts of text data to understand and generate human-like text. Some well-known examples of LLMs include GPT-3 (Generative Pre-trained Transformer 3) developed by OpenAI, BERT (Bidirectional Encoder Representations from Transformers) by Google, and LaMDA (Language Model for Dialogue Applications) by Google. LLMs can be used for various NLP tasks such as text generation, translation, summarization, question answering, and more. They have shown remarkable capabilities in understanding and producing human-like language, making them valuable tools in various applications and industries!!', additional_kwargs={'usage': {'prompt_tokens': 29, 'completion_tokens': 161, 'total_tokens': 190}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 29, 'completion_tokens': 161, 'total_tokens': 190}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-78f97c7b-be56-4b38-a5b8-3a8fc36cbbef-0', usage_metadata={'input_tokens': 29, 'output_tokens': 161, 'total_tokens': 190})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
