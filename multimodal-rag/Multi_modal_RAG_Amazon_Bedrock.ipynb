{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tabula\n",
    "import faiss\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import pymupdf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"attention_paper.pdf\"\n",
    "filepath = \"data/\" + filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages:  27%|██▋       | 3/11 [00:02<00:05,  1.52it/s]Oct 02, 2024 6:02:07 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
      "WARNING: No Unicode mapping for summationtext (80) in font THPNLT+CMEX9\n",
      "Processing PDF pages:  55%|█████▍    | 6/11 [00:02<00:01,  3.32it/s]Oct 02, 2024 6:02:07 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
      "WARNING: No Unicode mapping for epsilon1 (15) in font LICAEO+CMMI10\n",
      "Processing PDF pages:  64%|██████▎   | 7/11 [00:03<00:01,  3.52it/s]Oct 02, 2024 6:02:08 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
      "WARNING: No Unicode mapping for epsilon1 (15) in font LICAEO+CMMI10\n",
      "Processing PDF pages:  73%|███████▎  | 8/11 [00:03<00:00,  4.23it/s]Oct 02, 2024 6:02:08 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
      "WARNING: No Unicode mapping for epsilon1 (15) in font LICAEO+CMMI10\n",
      "Processing PDF pages: 100%|██████████| 11/11 [00:03<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import pdf2imgs\n",
    "\n",
    "doc = pymupdf.open(filepath)\n",
    "num_pages = len(doc)\n",
    "\n",
    "# Define the directories to store the extracted text, images and page images from each page\n",
    "image_save_dir = \"data/processed_images\"\n",
    "text_save_dir = \"data/processed_text\"\n",
    "table_save_dir = \"data/processed_tables\"\n",
    "page_images_save_dir = \"data/processed_page_images\"\n",
    "\n",
    "# Chunk the text for effective retrieval\n",
    "chunk_size = 700\n",
    "overlap=200\n",
    "\n",
    "# Process chunks with LangChain's RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=overlap,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "items = []\n",
    "# Process all pages of the PDF\n",
    "for page_num in tqdm(range(num_pages), desc=\"Processing PDF pages\"):\n",
    "    page = doc[page_num]\n",
    "    text = page.get_text()\n",
    "\n",
    "    # Step 1: Extract tables using Tabula\n",
    "    try:\n",
    "        tables = tabula.read_pdf(filepath, pages=page_num + 1, multiple_tables=True)\n",
    "        if tables:\n",
    "            for table_idx, table in enumerate(tables):\n",
    "                # Convert the table DataFrame to a string format (Markdown-style for clarity)\n",
    "                table_text = \"\\n\".join([\" | \".join(map(str, row)) for row in table.values])\n",
    "\n",
    "                # Save the table text as a file\n",
    "                table_file_name = f\"{table_save_dir}/{os.path.basename(filepath)}_table_{page_num}_{table_idx}.txt\"\n",
    "                os.makedirs(table_save_dir, exist_ok=True)\n",
    "                with open(table_file_name, 'w') as f:\n",
    "                    f.write(table_text)\n",
    "\n",
    "                # Add table information to items (format as a string for embeddings)\n",
    "                table_item = {\n",
    "                    \"page\": page_num,\n",
    "                    \"type\": \"table\",\n",
    "                    \"text\": table_text,  # Use the formatted table text here\n",
    "                    \"path\": table_file_name\n",
    "                }\n",
    "                items.append(table_item)\n",
    "\n",
    "                # Optionally remove table text from the page's text to avoid duplication\n",
    "                text = text.replace(table_text, \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables from page {page_num}: {str(e)}\")\n",
    "            \n",
    "    # Process chunks with overlap\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    # chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size-overlap)]\n",
    "    \n",
    "    # Generate an item to add to items\n",
    "    for i,chunk in enumerate(chunks):\n",
    "        text_file_name = f\"{text_save_dir}/{filename}_text_{page_num}_{i}.txt\"\n",
    "        # If the text folder doesn't exist, create one\n",
    "        os.makedirs(text_save_dir, exist_ok=True)\n",
    "        with open(text_file_name, 'w') as f:\n",
    "            f.write(chunk)\n",
    "        \n",
    "        item={}\n",
    "        item[\"page\"] = page_num\n",
    "        item[\"type\"] = \"text\"\n",
    "        item[\"text\"] = chunk\n",
    "        item[\"path\"] = text_file_name\n",
    "        items.append(item)\n",
    "    \n",
    "    \n",
    "    # Get all the images in the current page\n",
    "    images = page.get_images()\n",
    "    for idx, image in enumerate(images):        \n",
    "        # Extract the image data\n",
    "        xref = image[0]\n",
    "        pix = pymupdf.Pixmap(doc, xref)\n",
    "        pix.tobytes(\"png\")\n",
    "        # Create the image_name that includes the image path\n",
    "        image_name = f\"{image_save_dir}/{filename}_image_{page_num}_{idx}_{xref}.png\"\n",
    "        # If the image folder doesn't exist, create one\n",
    "        os.makedirs(image_save_dir, exist_ok=True)\n",
    "        # Save the image\n",
    "        pix.save(image_name)\n",
    "        \n",
    "        # Produce base64 string\n",
    "        with open(image_name, 'rb') as f:\n",
    "            image = base64.b64encode(f.read()).decode('utf8')\n",
    "        \n",
    "        item={}\n",
    "        item[\"page\"] = page_num\n",
    "        item[\"type\"] = \"image\"\n",
    "        item[\"path\"] = image_name\n",
    "        item[\"image\"] = image\n",
    "        items.append(item)\n",
    "\n",
    "# Save pdf pages as images\n",
    "page_images_save_dir = pdf2imgs(filepath, page_images_save_dir)\n",
    "\n",
    "for page_num in range(num_pages):\n",
    "    page_path = os.path.join(page_images_save_dir,  f\"page_{page_num:03d}.png\")\n",
    "    \n",
    "    # Produce base64 string\n",
    "    with open(image_name, 'rb') as f:\n",
    "        page_image = base64.b64encode(f.read()).decode('utf8')\n",
    "    \n",
    "    item = {}\n",
    "    item[\"page\"] = page_num\n",
    "    item[\"type\"] = \"page\"\n",
    "    item[\"path\"] = page_path\n",
    "    item[\"image\"] = page_image\n",
    "    items.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Generation Code\n",
    "def generate_multimodal_embeddings(prompt=None, image=None, output_embedding_length=384):\n",
    "    \"\"\"\n",
    "    Invoke the Amazon Titan Multimodal Embeddings model using AWS Bedrock runtime.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to provide to the model.\n",
    "        image (str): A base64-encoded image data.\n",
    "    Returns:\n",
    "        str: The model's response embedding.\n",
    "    \"\"\"\n",
    "    if not prompt and not image:\n",
    "        raise ValueError(\"Please provide either a text prompt, base64 image, or both as input\")\n",
    "    \n",
    "    # Initialize the Amazon Bedrock runtime client\n",
    "    client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "    model_id = \"amazon.titan-embed-image-v1\"\n",
    "    \n",
    "    body = {\"embeddingConfig\": {\"outputEmbeddingLength\": output_embedding_length}}\n",
    "    \n",
    "    if prompt:\n",
    "        body[\"inputText\"] = prompt\n",
    "    if image:\n",
    "        body[\"inputImage\"] = image\n",
    "\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(body),\n",
    "            accept=\"application/json\",\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "\n",
    "        # Process and return the response\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        return result.get(\"embedding\")\n",
    "\n",
    "    except ClientError as err:\n",
    "        print(f\"Couldn't invoke Titan embedding model. Error: {err.response['Error']['Message']}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 85/85 [00:23<00:00,  3.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set embedding vector dimension\n",
    "embedding_vector_dimension = 384\n",
    "\n",
    "# Generate embeddings for all items\n",
    "for item in tqdm(items, desc=\"Generating embeddings\"):\n",
    "    if item['type'] == 'text' or item['type'] == 'table':\n",
    "        # For text or table, use the formatted text representation\n",
    "        item['embedding'] = generate_multimodal_embeddings(prompt=item['text'], output_embedding_length=embedding_vector_dimension)\n",
    "    else:\n",
    "        # For images, use the base64-encoded image data\n",
    "        item['embedding'] = generate_multimodal_embeddings(image=item['image'], output_embedding_length=embedding_vector_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.array([item['embedding'] for item in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 384)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS Index\n",
    "index = faiss.IndexFlatL2(embedding_vector_dimension)\n",
    "index.reset() # Clear any pre-existing index\n",
    "index.add(np.array(all_embeddings, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_claude_3_multimodal(prompt, images, image_types):\n",
    "    \"\"\"\n",
    "    Invoke the Claude-3 multimodal model from Anthropic using AWS Bedrock runtime.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to provide to the model.\n",
    "        images (list): A list of base64-encoded image data.\n",
    "        image_types (list): A list of MIME types corresponding to the images.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response text.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid model name is provided.\n",
    "    \"\"\"\n",
    "    # Initialize the Amazon Bedrock runtime client\n",
    "    client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    # Prepare the multimodal prompt message\n",
    "    message_content = []\n",
    "\n",
    "    # Add each image to the message content\n",
    "    for image, img_type in zip(images, image_types):\n",
    "        message_content.append({\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": img_type,\n",
    "                \"data\": image,\n",
    "            },\n",
    "        })\n",
    "    message_content.append({\"type\": \"text\", \"text\": prompt})\n",
    "\n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 250,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message_content,\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(request_body),\n",
    "        )\n",
    "\n",
    "        # Process and return the response\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        return result['content'][0]['text']\n",
    "\n",
    "    except ClientError as err:\n",
    "        logger.error(\n",
    "            \"Couldn't invoke Claude 3 %s model. Here's why: %s: %s\",\n",
    "            model_id.split('.')[-1].capitalize(),\n",
    "            err.response[\"Error\"][\"Code\"],\n",
    "            err.response[\"Error\"][\"Message\"],\n",
    "        )\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_response(prompt, matched_items):\n",
    "    \n",
    "    # Create context\n",
    "    text_context = \"\"\n",
    "    image_context = []\n",
    "    \n",
    "    for item in matched_items:\n",
    "        if item['type'] == 'text':\n",
    "            text_context += str(item[\"page\"]) + \". \" + item['text'] + \"\\n\"\n",
    "        else:\n",
    "            image_context.append(item['image'])\n",
    "    \n",
    "    # Only 5 images are supported by Claude3 models\n",
    "    if len(image_context) > 5:\n",
    "        image_context = image_context[:5]\n",
    "    \n",
    "    final_prompt = f\"\"\"You are a helpful assistant for question answering.\n",
    "    The text context is relevant information retrieved.\n",
    "    The provided image(s) are relevant information retrieved.\n",
    "    \n",
    "    <context>\n",
    "    {text_context}\n",
    "    </context>\n",
    "    \n",
    "    Answer the following question using the relevant context and images.\n",
    "    \n",
    "    <question>\n",
    "    {prompt}\n",
    "    </question>\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    return invoke_claude_3_multimodal(final_prompt, image_context, ['image/png' for _ in image_context])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is the scaled-dot-product attention is calculated?\"\n",
    "\n",
    "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
    "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 19, 20, 23, 40])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_items = [items[index] for index in result.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_rag_response(query, matched_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the context, the scaled dot-product attention is calculated as follows:\n",
       "\n",
       "Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
       "\n",
       "Where:\n",
       "\n",
       "- Q is the matrix of queries \n",
       "- K is the matrix of keys\n",
       "- V is the matrix of values\n",
       "- d_k is the dimension of the keys\n",
       "\n",
       "The key steps are:\n",
       "\n",
       "1. Take the dot product of the query matrix Q with the transpose of the key matrix K^T. This gives a score showing the similarity between each query and key.\n",
       "\n",
       "2. Scale the scores by dividing by sqrt(d_k), where d_k is the dimension of the keys. This helps prevent the softmax function from being pushed into regions with extremely small gradients when d_k is large.\n",
       "\n",
       "3. Apply the softmax function to the scaled scores to obtain the attention weights.\n",
       "\n",
       "4. Multiply the attention weights with the value matrix V to get the weighted sum of the values, which are the attended outputs.\n",
       "\n",
       "The scaling factor 1/sqrt(d_k) is a key difference from the standard dot-product attention. It helps counteract the effect of large dot products when the key dimension d_k is high, which can make the softmax saturate and have tiny gradients."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to Table 2 in the provided context, the Transformer (big) model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German (EN-DE) translation task."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the BLEU score of the model in English to German translation (EN-DE)?\"\n",
    "\n",
    "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
    "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)\n",
    "result.flatten()\n",
    "matched_items = [items[index] for index in result.flatten()]\n",
    "response = generate_rag_response(query, matched_items)\n",
    "display.Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px; border-left: 6px solid #4682B4;\">\n",
    "  <p>Nice. We have seen a few example questions and answers. Let's try asking more questions. Some example questions are given below:</p>\n",
    "  <ul style=\"text-align: left;\">\n",
    "    <li>\"How long were the base and big models trained?\"</li>\n",
    "    <li>\"Which optimizer was used when training the models?\"</li>\n",
    "    <li>\"What is position-wise feed-forward neural network mentioned in the paper?\"</li>\n",
    "    <li>\"What is the BLEU score of the model in English to French translation (EN-FR)?\"</li>\n",
    "    <li>\"What is the BLEU score of the model in English to German translation (EN-DE)?\"</li>\n",
    "  </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
