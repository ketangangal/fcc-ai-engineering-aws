{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Multimodal RAG with Amazon Bedrock\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to implement a multi-modal Retrieval-Augmented Generation (RAG) system using Amazon Bedrock. Many documents contain a mixture of content types, including text and images. Traditional RAG applications often lose valuable information captured in images. With the emergence of Multimodal Large Language Models (MLLMs), we can now leverage both text and image data in our RAG systems.\n",
    "\n",
    "In this notebook, we'll explore one approach to multi-modal RAG (`Option 1`):\n",
    "\n",
    "1. Use multimodal embeddings (such as Amazon Titan) to embed both images and text\n",
    "2. Retrieve relevant information using similarity search\n",
    "3. Pass raw images and text chunks to a multimodal LLM for answer synthesis using Claude 3/3.5 Multimodal\n",
    "\n",
    "We'll use the following tools and technologies:\n",
    "\n",
    "- [LangChain](https://python.langchain.com/v0.2/docs/introduction/) to build a multimodal RAG system\n",
    "- [faiss](https://github.com/facebookresearch/faiss) for similarity search\n",
    "- [Claude 3/3.5 Multimodal](https://www.anthropic.com/news/claude-3-5-sonnet) for answer synthesis\n",
    "- [Amazon Titan Multimodal Embeddings](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html) for image embeddings\n",
    "- [Amazon Bedrock](https://aws.amazon.com/bedrock/) for accessing powerful AI models, like the ones above\n",
    "- [pymupdf](https://pymupdf.readthedocs.io/en/latest/) to parse images, text, and tables from documents (PDFs)\n",
    "- [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) for interacting with Amazon Bedrock\n",
    "\n",
    "This approach allows us to create a more comprehensive RAG system that can understand and utilize both textual and visual information from our documents.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have the following packages and dependencies installed:\n",
    "\n",
    "- Python 3.9 or later\n",
    "- langchain\n",
    "- boto3\n",
    "- faiss\n",
    "- pymupdf\n",
    "- tabula\n",
    "- tesseract\n",
    "- requests\n",
    "\n",
    "Let's get started with building our multi-modal RAG system using Amazon Bedrock!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Multimodal RAG with Amazon Bedrock](../img/multimodal-rag1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Importing the libs\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install jpype1 tabula-py PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tabula\n",
    "import faiss\n",
    "import json\n",
    "import base64\n",
    "import pymupdf\n",
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Data Loading\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the dataset - URL of the \"Attention Is All You Need\" paper (Replace it with the URL of the PDF file/dataset you want to download)\n",
    "url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
    "\n",
    "# Set the filename and filepath\n",
    "filename = \"attention_paper.pdf\"\n",
    "filepath = os.path.join(\"data\", filename)\n",
    "\n",
    "# Create the data directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(filepath, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"File downloaded successfully: {filepath}\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the PDF file\n",
    "display.IFrame(filepath, width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Data Extraction\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "doc = pymupdf.open(filepath)\n",
    "num_pages = len(doc)\n",
    "\n",
    "# Define the directories to store the extracted text, images and page images from each page\n",
    "image_save_dir = \"data/processed_images\"\n",
    "text_save_dir = \"data/processed_text\"\n",
    "table_save_dir = \"data/processed_tables\"\n",
    "page_images_save_dir = \"data/processed_page_images\"\n",
    "\n",
    "# Chunk the text for effective retrieval\n",
    "chunk_size = 700\n",
    "overlap= 200\n",
    "\n",
    "# Process chunks with LangChain's RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "                                                chunk_size=chunk_size,\n",
    "                                                chunk_overlap=overlap,\n",
    "                                                length_function=len,\n",
    "                                              )\n",
    "\n",
    "# Process all pages of the PDF\n",
    "items = []\n",
    "for page_num in tqdm(range(num_pages), desc=\"Processing PDF pages\"):\n",
    "    page = doc[page_num]\n",
    "    text = page.get_text()\n",
    "\n",
    "    # Step 1: Get/extract all TABLES in the curremt page and store \n",
    "    try:\n",
    "        tables = tabula.read_pdf(filepath, pages=page_num + 1, multiple_tables=True)\n",
    "\n",
    "        if tables:\n",
    "            for table_idx, table in enumerate(tables):\n",
    "                # Convert the table DataFrame to a string format (Markdown-style for clarity)\n",
    "                table_text = \"\\n\".join([\" | \".join(map(str, row)) for row in table.values])\n",
    "\n",
    "                # Save the table text as a file\n",
    "                table_file_name = f\"{table_save_dir}/{os.path.basename(filepath)}_table_{page_num}_{table_idx}.txt\"\n",
    "                os.makedirs(table_save_dir, exist_ok=True)\n",
    "                with open(table_file_name, 'w') as f:\n",
    "                    f.write(table_text)\n",
    "\n",
    "                # Add table information to items (format as a string for embeddings)\n",
    "                table_item = {\n",
    "                    \"page\": page_num,\n",
    "                    \"type\": \"table\",\n",
    "                    \"text\": table_text,  # Use the formatted table text here\n",
    "                    \"path\": table_file_name\n",
    "                }\n",
    "                items.append(table_item)\n",
    "\n",
    "                # Optionally remove table text from the page's text to avoid duplication\n",
    "                text = text.replace(table_text, \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables from page {page_num}: {str(e)}\")\n",
    "            \n",
    "    # Step 2: Get all TEXT chunks in the curremt page and store \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Generate an item to add to items\n",
    "    for i,chunk in enumerate(chunks):\n",
    "        text_file_name = f\"{text_save_dir}/{filename}_text_{page_num}_{i}.txt\"\n",
    "        # If the text folder doesn't exist, create one\n",
    "        os.makedirs(text_save_dir, exist_ok=True)\n",
    "        with open(text_file_name, 'w') as f:\n",
    "            f.write(chunk)\n",
    "        \n",
    "        item={}\n",
    "        item[\"page\"] = page_num\n",
    "        item[\"type\"] = \"text\"\n",
    "        item[\"text\"] = chunk\n",
    "        item[\"path\"] = text_file_name\n",
    "        items.append(item)\n",
    "    \n",
    "    \n",
    "    # Step 3: Get all the IMAGES in the current page and store\n",
    "    images = page.get_images()\n",
    "    for idx, image in enumerate(images):        \n",
    "        # Extract the image data\n",
    "        xref = image[0]\n",
    "        pix = pymupdf.Pixmap(doc, xref)\n",
    "        pix.tobytes(\"png\")\n",
    "        # Create the image_name that includes the image path\n",
    "        image_name = f\"{image_save_dir}/{filename}_image_{page_num}_{idx}_{xref}.png\"\n",
    "        # If the image folder doesn't exist, create one\n",
    "        os.makedirs(image_save_dir, exist_ok=True)\n",
    "        # Save the image\n",
    "        pix.save(image_name)\n",
    "        \n",
    "        # Produce base64 string\n",
    "        with open(image_name, 'rb') as f:\n",
    "            image = base64.b64encode(f.read()).decode('utf8')\n",
    "        \n",
    "        item={}\n",
    "        item[\"page\"] = page_num\n",
    "        item[\"type\"] = \"image\"\n",
    "        item[\"path\"] = image_name\n",
    "        item[\"image\"] = image\n",
    "        items.append(item)\n",
    "\n",
    "    # Step 4: Save each pdf pages as images\n",
    "    pix = page.get_pixmap()\n",
    "    page_path = os.path.join(page_images_save_dir, f\"page_{page_num:03d}.png\")\n",
    "    pix.save(page_path)\n",
    "    \n",
    "    # Produce base64 string\n",
    "    with open(page_path, 'rb') as f:\n",
    "        page_image = base64.b64encode(f.read()).decode('utf8')\n",
    "    \n",
    "    item = {}\n",
    "    item[\"page\"] = page_num\n",
    "    item[\"type\"] = \"page\"\n",
    "    item[\"path\"] = page_path\n",
    "    item[\"image\"] = page_image\n",
    "    items.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the first text item\n",
    "[i for i in items if i['type'] == 'text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking at the first table item\n",
    "[i for i in items if i['type'] == 'table'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking at the first image item\n",
    "[i for i in items if i['type'] == 'image'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Generating Multimodal Embeddings\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import boto3 \n",
    "\n",
    "# Generating Multimodal Embeddings using Amazon Titan Multimodal Embeddings model\n",
    "def generate_multimodal_embeddings(prompt=None, image=None, output_embedding_length=384):\n",
    "    \"\"\"\n",
    "    Invoke the Amazon Titan Multimodal Embeddings model using Amazon Bedrock runtime.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to provide to the model.\n",
    "        image (str): A base64-encoded image data.\n",
    "    Returns:\n",
    "        str: The model's response embedding.\n",
    "    \"\"\"\n",
    "    if not prompt and not image:\n",
    "        raise ValueError(\"Please provide either a text prompt, base64 image, or both as input\")\n",
    "    \n",
    "    # Initialize the Amazon Bedrock runtime client\n",
    "    client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "    model_id = \"amazon.titan-embed-image-v1\"\n",
    "    \n",
    "    body = {\"embeddingConfig\": {\"outputEmbeddingLength\": output_embedding_length}}\n",
    "    \n",
    "    if prompt:\n",
    "        body[\"inputText\"] = prompt\n",
    "    if image:\n",
    "        body[\"inputImage\"] = image\n",
    "\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(body),\n",
    "            accept=\"application/json\",\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "\n",
    "        # Process and return the response\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        return result.get(\"embedding\")\n",
    "\n",
    "    except ClientError as err:\n",
    "        print(f\"Couldn't invoke Titan embedding model. Error: {err.response['Error']['Message']}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding vector dimension\n",
    "embedding_vector_dimension = 384\n",
    "\n",
    "# Count the number of each type of item\n",
    "item_counts = {\n",
    "    'text': sum(1 for item in items if item['type'] == 'text'),\n",
    "    'table': sum(1 for item in items if item['type'] == 'table'),\n",
    "    'image': sum(1 for item in items if item['type'] == 'image'),\n",
    "    'page': sum(1 for item in items if item['type'] == 'page')\n",
    "}\n",
    "\n",
    "# Initialize counters\n",
    "counters = dict.fromkeys(item_counts.keys(), 0)\n",
    "\n",
    "# Generate embeddings for all items\n",
    "with tqdm(total=len(items), desc=\"Generating embeddings\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\") as pbar:\n",
    "    for item in items:\n",
    "        item_type = item['type']\n",
    "        counters[item_type] += 1\n",
    "        \n",
    "        if item_type in ['text', 'table']:\n",
    "            # For text or table, use the formatted text representation\n",
    "            item['embedding'] = generate_multimodal_embeddings(prompt=item['text'], output_embedding_length=embedding_vector_dimension)\n",
    "        else:\n",
    "            # For images, use the base64-encoded image data\n",
    "            item['embedding'] = generate_multimodal_embeddings(image=item['image'], output_embedding_length=embedding_vector_dimension)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        pbar.set_postfix_str(f\"Text: {counters['text']}/{item_counts['text']}, Table: {counters['table']}/{item_counts['table']}, Image: {counters['image']}/{item_counts['image']}\")\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Creating Vector Database/Index\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vector Database](data/vectordb.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# All the embeddings\n",
    "all_embeddings = np.array([item['embedding'] for item in items])\n",
    "\n",
    "# Create FAISS Index\n",
    "index = faiss.IndexFlatL2(embedding_vector_dimension)\n",
    "\n",
    "# Clear any pre-existing index\n",
    "index.reset()\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(np.array(all_embeddings, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Generating RAG response with Claude 3.5/3\n",
    "def invoke_claude_3_multimodal(prompt, images, image_types):\n",
    "    \"\"\"\n",
    "    Invoke the Claude-3 multimodal model from Anthropic using AWS Bedrock runtime.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to provide to the model.\n",
    "        images (list): A list of base64-encoded image data.\n",
    "        image_types (list): A list of MIME types corresponding to the images.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response text.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid model name is provided.\n",
    "    \"\"\"\n",
    "    # Initialize the Amazon Bedrock runtime client and model ID \n",
    "    client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "    # model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"     \n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"        # Replace with the model ID you want to use\n",
    "\n",
    "    # Prepare the multimodal prompt message\n",
    "    message_content = []\n",
    "\n",
    "    # Add each image to the message content\n",
    "    for image, img_type in zip(images, image_types):\n",
    "        message_content.append({\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": img_type,\n",
    "                \"data\": image,\n",
    "            },\n",
    "        })\n",
    "    message_content.append({\"type\": \"text\", \"text\": prompt})\n",
    "\n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 250,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message_content,\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(request_body),\n",
    "        )\n",
    "\n",
    "        # Process and return the response\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        return result['content'][0]['text']\n",
    "\n",
    "    except ClientError as err:\n",
    "        logger.error(\n",
    "            \"Couldn't invoke Claude 3 %s model. Here's why: %s: %s\",\n",
    "            model_id.split('.')[-1].capitalize(),\n",
    "            err.response[\"Error\"][\"Code\"],\n",
    "            err.response[\"Error\"][\"Message\"],\n",
    "        )\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Generate RAG response with Claude 3.5/3\n",
    "def generate_rag_response(prompt, matched_items):\n",
    "    \n",
    "    # Create context\n",
    "    text_context = \"\"\n",
    "    image_context = []\n",
    "    \n",
    "    for item in matched_items:\n",
    "        if item['type'] == 'text' or item['type'] == 'table':\n",
    "            text_context += str(item[\"page\"]) + \". \" + item['text'] + \"\\n\"\n",
    "        else:\n",
    "            image_context.append(item['image'])\n",
    "    \n",
    "    # Only 5 images are supported by Claude3 models\n",
    "    if len(image_context) > 5:\n",
    "        image_context = image_context[:5]\n",
    "    \n",
    "    final_prompt = f\"\"\"You are a helpful assistant for question answering.\n",
    "    The text context is relevant information retrieved.\n",
    "    The provided image(s) are relevant information retrieved.\n",
    "    \n",
    "    <context>\n",
    "    {text_context}\n",
    "    </context>\n",
    "    \n",
    "    Answer the following question using the relevant context and images.\n",
    "    \n",
    "    <question>\n",
    "    {prompt}\n",
    "    </question>\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "\n",
    "    response = invoke_claude_3_multimodal(final_prompt, image_context, ['image/png' for _ in image_context])\n",
    "    \n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Test the RAG Pipeline\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# User Query\n",
    "query = \"How is the scaled-dot-product attention is calculated?\"\n",
    "\n",
    "# Generate embeddings for the query\n",
    "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
    "\n",
    "# Search for the nearest neighbors in the vector database\n",
    "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Check the result (matched chunks)\n",
    "result.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the matched items\n",
    "matched_items = [items[index] for index in result.flatten()]\n",
    "\n",
    "# Generate a response using the RAG pipeline\n",
    "response = generate_rag_response(query, matched_items)\n",
    "\n",
    "# Display the response\n",
    "display.Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Query\n",
    "query = \"What is the BLEU score of the model in English to German translation (EN-DE)?\"\n",
    "\n",
    "# Generate embeddings for the query\n",
    "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
    "\n",
    "# Search for the nearest neighbors in the vector database\n",
    "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)\n",
    "result.flatten()\n",
    "\n",
    "# Retrieve the matched items\n",
    "matched_items = [items[index] for index in result.flatten()]\n",
    "\n",
    "# Generate a response using the RAG pipeline\n",
    "response = generate_rag_response(query, matched_items)\n",
    "\n",
    "# Display the response\n",
    "display.Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Your Turn: Test the RAG Pipeline\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# List of queries (Replace with any query of your choice)\n",
    "other_queries = [\"How long were the base and big models trained?\",\n",
    "                 \"Which optimizer was used when training the models?\",\n",
    "                 \"What is the position-wise feed-forward neural network mentioned in the paper?\",\n",
    "                 \"What is the BLEU score of the model in English to French translation (EN-FR)?\",\n",
    "                 \"What is the BLEU score of the model in English to German translation (EN-DE)?\",\n",
    "                 \"How is the scaled-dot-product attention is calculated?\",\n",
    "                 \"What is the BLEU score of the model in English to French translation (EN-FR)?\",\n",
    "                 \"What is the BLEU score of the model in English to German translation (EN-DE)?\",\n",
    "                 \"How is the scaled-dot-product attention is calculated?\",\n",
    "                 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "query = other_queries[0] # Replace with any query from the list above\n",
    "\n",
    "# Generate embeddings for the query\n",
    "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
    "\n",
    "# Search for the nearest neighbors in the vector database\n",
    "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)\n",
    "result.flatten()\n",
    "\n",
    "# Retrieve the matched items\n",
    "matched_items = [items[index] for index in result.flatten()]\n",
    "\n",
    "# Generate a response using the RAG pipeline\n",
    "response = generate_rag_response(query, matched_items)\n",
    "\n",
    "# Display the response\n",
    "display.Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
