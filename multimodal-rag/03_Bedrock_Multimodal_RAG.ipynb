{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8fddcb2-5374-4425-98d8-a3e35aaa33de",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Multimodal RAG\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d7307",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px;\">\n",
    "  <br>\n",
    "  <hr>\n",
    "\n",
    "  <p>This exercise shows how to implement a <b>multimodal retrieval augmented generation (RAG)</b> system. In retrieval augmented generation, an external source of information as well as the input prompt are used to generate the response. In a multimodal setting, one of the most popular use cases is to include the <b>images</b> in the response generation process.</p>\n",
    "  \n",
    "  <p>This exercise implements the multimodal RAG system by using a <b>PDF file</b> that includes images, text, and tables. This PDF file is the external source of information mentioned earlier in the RAG definition. Once the system is set up, the model will be able to generate its responses considering the images, text, and tables from the PDF provided.</p>\n",
    "\n",
    "  <p>Here is the list of topics covered in this exercise:</p>\n",
    "  <ol>\n",
    "    <li>Installing dependencies</li>\n",
    "    <li>Process PDF</li>\n",
    "    <li>Generate multimodal embeddings</li>\n",
    "    <li>Create vector database</li>\n",
    "    <li>Generate a RAG Response</li>\n",
    "    <li>Test RAG Workflow</li>\n",
    "  </ol>\n",
    "\n",
    "  <hr>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2cde53-8027-4bf4-89f5-fa5fcad94d0d",
   "metadata": {},
   "source": [
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4f16d",
   "metadata": {},
   "source": [
    "Installing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca6926e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ec7fb4",
   "metadata": {},
   "source": [
    "Importing the libraries used in this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a6fb80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import base64\n",
    "import pymupdf\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cdc15a",
   "metadata": {},
   "source": [
    "### Process PDF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc293ed-9b55-4be8-8cf2-e0f89ed569bb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px; border-left: 6px solid #4682B4;\">\n",
    "  <p>In this lab, we will read the sample PDF file of the well-known paper <b>“Attention Is All You Need”</b> by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. This research paper laid out the foundations of the <b>transformer models</b> that power many generative AI applications nowadays. Paper linked <a href=\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\">here</a>. The paper is <b>11 pages</b> long.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d39362-1ec7-4e61-a4a0-caad0abb7fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"600\"\n",
       "            src=\"data/attention_paper.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7efed049b520>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"attention_paper.pdf\"\n",
    "filepath = \"data/\" + filename\n",
    "display.IFrame(filepath, width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e740cd9-9efe-4e2c-9e89-fdf0247b6033",
   "metadata": {},
   "source": [
    "### Extract text and images from each page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55771b-1f60-4dab-b317-3e8de647a150",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px; border-left: 6px solid #4682B4;\">\n",
    "  <p>The contents of the PDF need to be extracted and processed to be compatible with the <b>RAG application</b>.</p>\n",
    "  \n",
    "  <p>The following are the steps we will follow to process the data in this section:</p>\n",
    "  <ol>\n",
    "    <li>Extract the data (text and images) from the PDF using <b>pymupdf</b>.</li>\n",
    "    <li>Go through each page. Create smaller text chunks from the text of the page.</li>\n",
    "    <li>Convert each page of the PDF into an image.</li>\n",
    "    <li>For each text chunk, image, and page, generate embeddings using <b>Amazon Titan Multimodal</b>.</li>\n",
    "    <li>Save the information of each page in a list to store in a <b>vector database</b>.</li>\n",
    "  </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432f994f-13ca-4688-9dc4-1513d6581e81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages: 100%|██████████| 11/11 [00:00<00:00, 34.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import pdf2imgs\n",
    "\n",
    "doc = pymupdf.open(filepath)\n",
    "num_pages = len(doc)\n",
    "\n",
    "# Define the directories to store the extracted text, images and page images from each page\n",
    "image_save_dir = \"data/images\"\n",
    "text_save_dir = \"data/text\"\n",
    "page_images_save_dir = \"data/page_images\"\n",
    "\n",
    "# Chunk the text for effective retrieval\n",
    "chunk_size = 700\n",
    "overlap=200\n",
    "\n",
    "\n",
    "items = []\n",
    "# Process all pages of the PDF\n",
    "for page_num in tqdm(range(num_pages), desc=\"Processing PDF pages\"):\n",
    "    page = page = doc[page_num]\n",
    "    text = page.get_text()\n",
    "    \n",
    "    # Process chunks with overlap\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size-overlap)]\n",
    "    \n",
    "    # Generate an item to add to items\n",
    "    for i,chunk in enumerate(chunks):\n",
    "        text_file_name = f\"{text_save_dir}/{filename}_text_{page_num}_{i}.txt\"\n",
    "        # If the text folder doesn't exist, create one\n",
    "        os.makedirs(text_save_dir, exist_ok=True)\n",
    "        with open(text_file_name, 'w') as f:\n",
    "            f.write(chunk)\n",
    "        \n",
    "        item={}\n",
    "        item[\"page\"] = page_num\n",
    "        item[\"type\"] = \"text\"\n",
    "        item[\"text\"] = chunk\n",
    "        item[\"path\"] = text_file_name\n",
    "        items.append(item)\n",
    "    \n",
    "    \n",
    "    # Get all the images in the current page\n",
    "    images = page.get_images()\n",
    "    for idx, image in enumerate(images):        \n",
    "        # Extract the image data\n",
    "        xref = image[0]\n",
    "        pix = pymupdf.Pixmap(doc, xref)\n",
    "        pix.tobytes(\"png\")\n",
    "        # Create the image_name that includes the image path\n",
    "        image_name = f\"{image_save_dir}/{filename}_image_{page_num}_{idx}_{xref}.png\"\n",
    "        # If the image folder doesn't exist, create one\n",
    "        os.makedirs(image_save_dir, exist_ok=True)\n",
    "        # Save the image\n",
    "        pix.save(image_name)\n",
    "        \n",
    "        # Produce base64 string\n",
    "        with open(image_name, 'rb') as f:\n",
    "            image = base64.b64encode(f.read()).decode('utf8')\n",
    "        \n",
    "        item={}\n",
    "        item[\"page\"] = page_num\n",
    "        item[\"type\"] = \"image\"\n",
    "        item[\"path\"] = image_name\n",
    "        item[\"image\"] = image\n",
    "        items.append(item)\n",
    "\n",
    "# Save pdf pages as images\n",
    "page_images_save_dir = pdf2imgs(filepath, page_images_save_dir)\n",
    "\n",
    "for page_num in range(num_pages):\n",
    "    page_path = os.path.join(page_images_save_dir,  f\"page_{page_num:03d}.png\")\n",
    "    \n",
    "    # Produce base64 string\n",
    "    with open(image_name, 'rb') as f:\n",
    "        page_image = base64.b64encode(f.read()).decode('utf8')\n",
    "    \n",
    "    item = {}\n",
    "    item[\"page\"] = page_num\n",
    "    item[\"type\"] = \"page\"\n",
    "    item[\"path\"] = page_path\n",
    "    item[\"image\"] = page_image\n",
    "    items.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d596bf-5409-4a23-b4da-f37cbff85c8a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px; border-left: 6px solid #4682B4;\">\n",
    "  <p>In the cell above, we have used a simple <b>character-based chunking</b> solution. This can result in broken sentences and words, losing a lot of <b>semantic</b> and <b>syntactic meaning</b>. Try updating the chunking process to preserve the structure and meaning of the document. You can use modules offered by <b>LangChain</b> for this purpose.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0709610b-ce9d-4667-a8a7-b45b1521c988",
   "metadata": {},
   "source": [
    "### Generate Multimodal Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee4820-e680-4d5c-b8a4-e147d01cada7",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px; border-left: 6px solid #4682B4;\">\n",
    "  <p>We will use the same function defined in <b>Lab 2</b> to generate embeddings from <b>text</b> or <b>image data</b>.</p>\n",
    "\n",
    "  <p>The following function is used to generate <b>multimodal embeddings</b> using <b>Amazon's Titan Multimodal Embeddings model</b>. Embeddings can be generated with <b>text data</b>, <b>image data</b>, or <b>both</b>.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f157f9-6eda-4206-aaa0-450fbe705d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_multimodal_embeddings(prompt=None, image=None, output_embedding_length = 384):\n",
    "    \"\"\"\n",
    "    Invoke the Amazon Titan Multimodal Embeddings model using AWS Bedrock runtime.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to provide to the model.\n",
    "        image (str): A base64-encoded image data.\n",
    "    Returns:\n",
    "        str: The model's response text.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid model name is provided.\n",
    "    \"\"\"\n",
    "    if not prompt and not image:\n",
    "        raise ValueError(\"Please provide either a text prompt, base64 image or both as input\")\n",
    "    \n",
    "    # Initialize the Amazon Bedrock runtime client\n",
    "    client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "    model_id = \"amazon.titan-embed-image-v1\"\n",
    "    \n",
    "    body = {\"embeddingConfig\": {\"outputEmbeddingLength\": output_embedding_length}}\n",
    "    \n",
    "    if prompt:\n",
    "        body[\"inputText\"] = prompt\n",
    "    if image:\n",
    "        body[\"inputImage\"] = image\n",
    "\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(body),\n",
    "            accept = \"application/json\",\n",
    "            contentType = \"application/json\"\n",
    "        )\n",
    "\n",
    "        # Process and return the response\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        return result.get(\"embedding\")\n",
    "\n",
    "    except ClientError as err:\n",
    "        logger.error(\n",
    "            \"Couldn't invoke Titan embedding %s model. Here's why: %s: %s\",\n",
    "            model.capitalize(),\n",
    "            err.response[\"Error\"][\"Code\"],\n",
    "            err.response[\"Error\"][\"Message\"],\n",
    "        )\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe25b18-fd39-4e2d-955c-c079c6c54083",
   "metadata": {},
   "source": [
    "#### Let's use the `generate_multimodal_embeddings` function to generate embeddings of every item extracted from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f4f7b1b-befb-4458-a0bd-348581ee9b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 85/85 [00:09<00:00,  8.67it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_dimension = 384\n",
    "for item in tqdm(items, \"Generating embeddings\"):\n",
    "    if item['type'] == 'text':\n",
    "        item['embedding'] = generate_multimodal_embeddings(prompt=item['text'], output_embedding_length=embedding_vector_dimension)\n",
    "    else:\n",
    "        item['embedding'] = generate_multimodal_embeddings(image=item['image'], output_embedding_length=embedding_vector_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d8ba0-b751-4c95-b944-dc02b1359a29",
   "metadata": {},
   "source": [
    "### Create vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c5aac2-f628-434f-8ca2-8d5f297a194b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px; border-left: 6px solid #4682B4;\">\n",
    "  <p>In this section, we will create an index using <b>FAISS</b>, similar to <b>Lab 2</b>. We will create a <a href=\"https://www.pinecone.io/learn/series/faiss/faiss-tutorial/#IndexFlatL2\"><b>FlatIndex</b></a> which measures the L2 (or <b>Euclidean</b>) distance between all given points between our query vector and the vectors loaded into the index.</p>\n",
    "\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"data/vectordb.png\" width=\"500\"/>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a54fd849-a046-4ccd-a918-0ca3911c473a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_embeddings = np.array([item['embedding'] for item in items])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a59cc-359c-4b46-9667-9a065e9e497e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px; border-left: 6px solid #4682B4;\">\n",
    "  <p>Now, we will use <code>FlatIndexL2</code> as the index type for the <b>vector database</b>. You may like to use a different index and observe how the <b>speed</b> and the <b>quality of results</b> change.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c131fe1-a511-46ba-818e-a91163bdb338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create FAISS Index\n",
    "index = faiss.IndexFlatL2(embedding_vector_dimension)\n",
    "index.reset() # Clear any pre-existing index\n",
    "index.add(np.array(all_embeddings, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525ab8c-2c1f-473d-855f-a36d14f47e48",
   "metadata": {},
   "source": [
    "### Generate a RAG Response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a2c08-d38f-4609-877f-c5ac375b6fd5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px; border-left: 6px solid #4682B4;\">\n",
    "  <p>In this section, we will define the function <code>generate_rag_response</code> to generate a response with a <b>retrieval-augmented prompt</b>.</p>\n",
    "\n",
    "  <p>First, let's define the <code>invoke_claude_3_multimodal</code> function that we used in <b>Lab 1</b> to generate a response to a <b>multimodal prompt</b>.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "580d4b25-bda1-49ed-aacf-9fae3bdb8f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_claude_3_multimodal(prompt, images, image_types):\n",
    "    \"\"\"\n",
    "    Invoke the Claude-3 multimodal model from Anthropic using AWS Bedrock runtime.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to provide to the model.\n",
    "        images (list): A list of base64-encoded image data.\n",
    "        image_types (list): A list of MIME types corresponding to the images.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response text.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid model name is provided.\n",
    "    \"\"\"\n",
    "    # Initialize the Amazon Bedrock runtime client\n",
    "    client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    # Prepare the multimodal prompt message\n",
    "    message_content = []\n",
    "\n",
    "    # Add each image to the message content\n",
    "    for image, img_type in zip(images, image_types):\n",
    "        message_content.append({\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": img_type,\n",
    "                \"data\": image,\n",
    "            },\n",
    "        })\n",
    "    message_content.append({\"type\": \"text\", \"text\": prompt})\n",
    "\n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 250,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message_content,\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(request_body),\n",
    "        )\n",
    "\n",
    "        # Process and return the response\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        return result['content'][0]['text']\n",
    "\n",
    "    except ClientError as err:\n",
    "        logger.error(\n",
    "            \"Couldn't invoke Claude 3 %s model. Here's why: %s: %s\",\n",
    "            model_id.split('.')[-1].capitalize(),\n",
    "            err.response[\"Error\"][\"Code\"],\n",
    "            err.response[\"Error\"][\"Message\"],\n",
    "        )\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9c037-62bf-43be-804e-11e7eb64b087",
   "metadata": {},
   "source": [
    "The following function, `generate_rag_response`, generates a prompt containing the user query, retrieved items and invokes the LLM to generate a RAG response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c8af8a3-9bd9-41d2-8fc4-606315ce3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_response(prompt, matched_items):\n",
    "    \n",
    "    # Create context\n",
    "    text_context = \"\"\n",
    "    image_context = []\n",
    "    \n",
    "    for item in matched_items:\n",
    "        if item['type'] == 'text':\n",
    "            text_context += str(item[\"page\"]) + \". \" + item['text'] + \"\\n\"\n",
    "        else:\n",
    "            image_context.append(item['image'])\n",
    "    \n",
    "    # Only 5 images are supported by Claude3 models\n",
    "    if len(image_context) > 5:\n",
    "        image_context = image_context[:5]\n",
    "    \n",
    "    final_prompt = f\"\"\"You are a helpful assistant for question answering.\n",
    "    The text context is relevant information retrieved.\n",
    "    The provided image(s) are relevant information retrieved.\n",
    "    \n",
    "    <context>\n",
    "    {text_context}\n",
    "    </context>\n",
    "    \n",
    "    Answer the following question using the relevant context and images.\n",
    "    \n",
    "    <question>\n",
    "    {prompt}\n",
    "    </question>\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    return invoke_claude_3_multimodal(final_prompt, image_context, ['image/png' for _ in image_context])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67011cf8-c1fd-4f7a-abdf-95703b0df611",
   "metadata": {},
   "source": [
    "### Test RAG Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c591c5b6-e54f-49b5-a13f-2f4f7c01f0b0",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px; border-left: 6px solid #4682B4;\">\n",
    "  <p>Now that we have our functions ready, let's test our <b>RAG application</b> using a few prompts.</p>\n",
    "\n",
    "  <p>The steps we follow to generate a <b>RAG response</b> are:</p>\n",
    "  <ol>\n",
    "    <li>Generate an embedding of the user query. The embedding would represent the <b>text</b> and the <b>images</b> provided in the user query.</li>\n",
    "    <li>Retrieve similar items from the vector database using a <b>nearest neighbor</b> search.</li>\n",
    "    <li>Create a prompt using the user query as well as the retrieved items.</li>\n",
    "    <li>Generate a response using the <b>retrieval-augmented prompt</b>.</li>\n",
    "  </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bad55425-f34a-43d9-a90f-afc4a8ed2e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How is the scaled-dot-product attention is calculated?\"\n",
    "\n",
    "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
    "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e5cdb5d-396b-4296-a28a-fc59e6c5a278",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 20, 21, 22, 55])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e288638e-3534-4b59-b654-9260883e669b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matched_items = [items[index] for index in result.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d66cc44-141c-4282-a7ad-b4585f8fd284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = generate_rag_response(query, matched_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d976e48e-4945-4678-8706-d105ffd6b68b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the provided context, the scaled dot-product attention is calculated as follows:\n",
       "\n",
       "Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
       "\n",
       "Where:\n",
       "\n",
       "- Q is the matrix of queries \n",
       "- K is the matrix of keys\n",
       "- V is the matrix of values\n",
       "- d_k is the dimension of the keys\n",
       "\n",
       "The key steps are:\n",
       "\n",
       "1. Take the dot product of the query matrix Q with the transpose of the key matrix K^T. This gives the similarity scores between each query and key.\n",
       "\n",
       "2. Scale the similarity scores by dividing by sqrt(d_k), where d_k is the dimension of the keys. This helps prevent extremely small gradients for large values of d_k.\n",
       "\n",
       "3. Apply the softmax function to the scaled scores to obtain the attention weights.\n",
       "\n",
       "4. Multiply the attention weights with the value matrix V to get the weighted sum of values, which are the attended outputs.\n",
       "\n",
       "So in essence, it computes the dot product similarities between queries and keys, scales them, converts to a probability distribution via softmax, and uses that to compute a weighted sum of the values. The scaling factor sqrt(d_k) helps stabilize the softmax computation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae56640",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; padding: 15px; border-radius: 10px; border-left: 6px solid #4682B4;\">\n",
    "  <p>Nice. We have seen a few example questions and answers. Let's try asking more questions. Some example questions are given below:</p>\n",
    "  <ul style=\"text-align: left;\">\n",
    "    <li>\"How long were the base and big models trained?\"</li>\n",
    "    <li>\"Which optimizer was used when training the models?\"</li>\n",
    "    <li>\"What is position-wise feed-forward neural network mentioned in the paper?\"</li>\n",
    "    <li>\"What is the BLEU score of the model in English to French translation (EN-FR)?\"</li>\n",
    "    <li>\"What is the BLEU score of the model in English to German translation (EN-DE)?\"</li>\n",
    "  </ul>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
