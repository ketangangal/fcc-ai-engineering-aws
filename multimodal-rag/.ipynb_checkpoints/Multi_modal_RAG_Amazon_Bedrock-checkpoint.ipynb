{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Multimodal RAG with Amazon Bedrock\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Multimodal RAG with Amazon Bedrock](../img/multimodal-rag1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Importing the libs\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tabula\n",
    "import faiss\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import pymupdf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Data Loading\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"attention_paper.pdf\"\n",
    "filepath = \"data/\" + filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"data/attention_paper.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x12cd7d3d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"attention_paper.pdf\"\n",
    "filepath = \"data/\" + filename\n",
    "display.IFrame(filepath, width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Data Extraction\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages:   0%|                              | 0/11 [00:00<?, ?it/s]OpenJDK 64-Bit Server VM warning: Attempt to protect stack guard pages failed.\n",
      "OpenJDK 64-Bit Server VM warning: Attempt to deallocate stack guard pages failed.\n",
      "Processing PDF pages:  27%|██████                | 3/11 [00:02<00:05,  1.37it/s]Oct 02, 2024 10:52:23 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
      "WARNING: No Unicode mapping for summationtext (80) in font THPNLT+CMEX9\n",
      "Processing PDF pages:  55%|████████████          | 6/11 [00:03<00:01,  3.06it/s]Oct 02, 2024 10:52:24 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
      "WARNING: No Unicode mapping for epsilon1 (15) in font LICAEO+CMMI10\n",
      "Processing PDF pages:  64%|██████████████        | 7/11 [00:03<00:01,  3.43it/s]Oct 02, 2024 10:52:24 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
      "WARNING: No Unicode mapping for epsilon1 (15) in font LICAEO+CMMI10\n",
      "Processing PDF pages:  73%|████████████████      | 8/11 [00:03<00:00,  4.31it/s]Oct 02, 2024 10:52:24 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
      "WARNING: No Unicode mapping for epsilon1 (15) in font LICAEO+CMMI10\n",
      "Processing PDF pages: 100%|█████████████████████| 11/11 [00:03<00:00,  2.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import pdf2imgs\n",
    "\n",
    "doc = pymupdf.open(filepath)\n",
    "num_pages = len(doc)\n",
    "\n",
    "# Define the directories to store the extracted text, images and page images from each page\n",
    "image_save_dir = \"data/processed_images\"\n",
    "text_save_dir = \"data/processed_text\"\n",
    "table_save_dir = \"data/processed_tables\"\n",
    "page_images_save_dir = \"data/processed_page_images\"\n",
    "\n",
    "# Chunk the text for effective retrieval\n",
    "chunk_size = 700\n",
    "overlap=200\n",
    "\n",
    "# Process chunks with LangChain's RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=overlap,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Process all pages of the PDF\n",
    "items = []\n",
    "for page_num in tqdm(range(num_pages), desc=\"Processing PDF pages\"):\n",
    "    page = doc[page_num]\n",
    "    text = page.get_text()\n",
    "\n",
    "    # Step 1: Extract tables using Tabula\n",
    "    try:\n",
    "        tables = tabula.read_pdf(filepath, pages=page_num + 1, multiple_tables=True)\n",
    "        if tables:\n",
    "            for table_idx, table in enumerate(tables):\n",
    "                # Convert the table DataFrame to a string format (Markdown-style for clarity)\n",
    "                table_text = \"\\n\".join([\" | \".join(map(str, row)) for row in table.values])\n",
    "\n",
    "                # Save the table text as a file\n",
    "                table_file_name = f\"{table_save_dir}/{os.path.basename(filepath)}_table_{page_num}_{table_idx}.txt\"\n",
    "                os.makedirs(table_save_dir, exist_ok=True)\n",
    "                with open(table_file_name, 'w') as f:\n",
    "                    f.write(table_text)\n",
    "\n",
    "                # Add table information to items (format as a string for embeddings)\n",
    "                table_item = {\n",
    "                    \"page\": page_num,\n",
    "                    \"type\": \"table\",\n",
    "                    \"text\": table_text,  # Use the formatted table text here\n",
    "                    \"path\": table_file_name\n",
    "                }\n",
    "                items.append(table_item)\n",
    "\n",
    "                # Optionally remove table text from the page's text to avoid duplication\n",
    "                text = text.replace(table_text, \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables from page {page_num}: {str(e)}\")\n",
    "            \n",
    "    # Process chunks with overlap\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    # chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size-overlap)]\n",
    "    \n",
    "    # Generate an item to add to items\n",
    "    for i,chunk in enumerate(chunks):\n",
    "        text_file_name = f\"{text_save_dir}/{filename}_text_{page_num}_{i}.txt\"\n",
    "        # If the text folder doesn't exist, create one\n",
    "        os.makedirs(text_save_dir, exist_ok=True)\n",
    "        with open(text_file_name, 'w') as f:\n",
    "            f.write(chunk)\n",
    "        \n",
    "        item={}\n",
    "        item[\"page\"] = page_num\n",
    "        item[\"type\"] = \"text\"\n",
    "        item[\"text\"] = chunk\n",
    "        item[\"path\"] = text_file_name\n",
    "        items.append(item)\n",
    "    \n",
    "    \n",
    "    # Get all the images in the current page\n",
    "    images = page.get_images()\n",
    "    for idx, image in enumerate(images):        \n",
    "        # Extract the image data\n",
    "        xref = image[0]\n",
    "        pix = pymupdf.Pixmap(doc, xref)\n",
    "        pix.tobytes(\"png\")\n",
    "        # Create the image_name that includes the image path\n",
    "        image_name = f\"{image_save_dir}/{filename}_image_{page_num}_{idx}_{xref}.png\"\n",
    "        # If the image folder doesn't exist, create one\n",
    "        os.makedirs(image_save_dir, exist_ok=True)\n",
    "        # Save the image\n",
    "        pix.save(image_name)\n",
    "        \n",
    "        # Produce base64 string\n",
    "        with open(image_name, 'rb') as f:\n",
    "            image = base64.b64encode(f.read()).decode('utf8')\n",
    "        \n",
    "        item={}\n",
    "        item[\"page\"] = page_num\n",
    "        item[\"type\"] = \"image\"\n",
    "        item[\"path\"] = image_name\n",
    "        item[\"image\"] = image\n",
    "        items.append(item)\n",
    "\n",
    "# Save pdf pages as images\n",
    "page_images_save_dir = pdf2imgs(filepath, page_images_save_dir)\n",
    "\n",
    "for page_num in range(num_pages):\n",
    "    page_path = os.path.join(page_images_save_dir,  f\"page_{page_num:03d}.png\")\n",
    "    \n",
    "    # Produce base64 string\n",
    "    with open(image_name, 'rb') as f:\n",
    "        page_image = base64.b64encode(f.read()).decode('utf8')\n",
    "    \n",
    "    item = {}\n",
    "    item[\"page\"] = page_num\n",
    "    item[\"type\"] = \"page\"\n",
    "    item[\"path\"] = page_path\n",
    "    item[\"image\"] = page_image\n",
    "    items.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Generating Multimodal Embeddings\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Embedding Generation Code\n",
    "def generate_multimodal_embeddings(prompt=None, image=None, output_embedding_length=384):\n",
    "    \"\"\"\n",
    "    Invoke the Amazon Titan Multimodal Embeddings model using AWS Bedrock runtime.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to provide to the model.\n",
    "        image (str): A base64-encoded image data.\n",
    "    Returns:\n",
    "        str: The model's response embedding.\n",
    "    \"\"\"\n",
    "    if not prompt and not image:\n",
    "        raise ValueError(\"Please provide either a text prompt, base64 image, or both as input\")\n",
    "    \n",
    "    # Initialize the Amazon Bedrock runtime client\n",
    "    client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "    model_id = \"amazon.titan-embed-image-v1\"\n",
    "    \n",
    "    body = {\"embeddingConfig\": {\"outputEmbeddingLength\": output_embedding_length}}\n",
    "    \n",
    "    if prompt:\n",
    "        body[\"inputText\"] = prompt\n",
    "    if image:\n",
    "        body[\"inputImage\"] = image\n",
    "\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(body),\n",
    "            accept=\"application/json\",\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "\n",
    "        # Process and return the response\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        return result.get(\"embedding\")\n",
    "\n",
    "    except ClientError as err:\n",
    "        print(f\"Couldn't invoke Titan embedding model. Error: {err.response['Error']['Message']}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|████████████████████| 85/85 [00:23<00:00,  3.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set embedding vector dimension\n",
    "embedding_vector_dimension = 384\n",
    "\n",
    "# Generate embeddings for all items\n",
    "for item in tqdm(items, desc=\"Generating embeddings\"):\n",
    "    if item['type'] == 'text' or item['type'] == 'table':\n",
    "        # For text or table, use the formatted text representation\n",
    "        item['embedding'] = generate_multimodal_embeddings(prompt=item['text'], output_embedding_length=embedding_vector_dimension)\n",
    "    else:\n",
    "        # For images, use the base64-encoded image data\n",
    "        item['embedding'] = generate_multimodal_embeddings(image=item['image'], output_embedding_length=embedding_vector_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "all_embeddings = np.array([item['embedding'] for item in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 384)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Creating Vector Database\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create FAISS Index\n",
    "index = faiss.IndexFlatL2(embedding_vector_dimension)\n",
    "\n",
    "# Clear any pre-existing index\n",
    "index.reset()\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(np.array(all_embeddings, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Generating RAG response with Claude 3\n",
    "def invoke_claude_3_multimodal(prompt, images, image_types):\n",
    "    \"\"\"\n",
    "    Invoke the Claude-3 multimodal model from Anthropic using AWS Bedrock runtime.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to provide to the model.\n",
    "        images (list): A list of base64-encoded image data.\n",
    "        image_types (list): A list of MIME types corresponding to the images.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response text.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid model name is provided.\n",
    "    \"\"\"\n",
    "    # Initialize the Amazon Bedrock runtime client\n",
    "    client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    # Prepare the multimodal prompt message\n",
    "    message_content = []\n",
    "\n",
    "    # Add each image to the message content\n",
    "    for image, img_type in zip(images, image_types):\n",
    "        message_content.append({\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": img_type,\n",
    "                \"data\": image,\n",
    "            },\n",
    "        })\n",
    "    message_content.append({\"type\": \"text\", \"text\": prompt})\n",
    "\n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 250,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message_content,\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(request_body),\n",
    "        )\n",
    "\n",
    "        # Process and return the response\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        return result['content'][0]['text']\n",
    "\n",
    "    except ClientError as err:\n",
    "        logger.error(\n",
    "            \"Couldn't invoke Claude 3 %s model. Here's why: %s: %s\",\n",
    "            model_id.split('.')[-1].capitalize(),\n",
    "            err.response[\"Error\"][\"Code\"],\n",
    "            err.response[\"Error\"][\"Message\"],\n",
    "        )\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_rag_response(prompt, matched_items):\n",
    "    \n",
    "    # Create context\n",
    "    text_context = \"\"\n",
    "    image_context = []\n",
    "    \n",
    "    for item in matched_items:\n",
    "        if item['type'] == 'text':\n",
    "            text_context += str(item[\"page\"]) + \". \" + item['text'] + \"\\n\"\n",
    "        else:\n",
    "            image_context.append(item['image'])\n",
    "    \n",
    "    # Only 5 images are supported by Claude3 models\n",
    "    if len(image_context) > 5:\n",
    "        image_context = image_context[:5]\n",
    "    \n",
    "    final_prompt = f\"\"\"You are a helpful assistant for question answering.\n",
    "    The text context is relevant information retrieved.\n",
    "    The provided image(s) are relevant information retrieved.\n",
    "    \n",
    "    <context>\n",
    "    {text_context}\n",
    "    </context>\n",
    "    \n",
    "    Answer the following question using the relevant context and images.\n",
    "    \n",
    "    <question>\n",
    "    {prompt}\n",
    "    </question>\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    return invoke_claude_3_multimodal(final_prompt, image_context, ['image/png' for _ in image_context])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Test the RAG Pipeline\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "query = \"How is the scaled-dot-product attention is calculated?\"\n",
    "\n",
    "# Generate embeddings for the query\n",
    "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
    "\n",
    "# Search for the nearest neighbors in the vector database\n",
    "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 19, 20, 23, 40])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the provided context, the scaled dot-product attention is calculated as follows:\n",
       "\n",
       "Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
       "\n",
       "Where:\n",
       "\n",
       "- Q is the matrix of queries \n",
       "- K is the matrix of keys\n",
       "- V is the matrix of values\n",
       "- d_k is the dimension of the keys\n",
       "\n",
       "The key steps are:\n",
       "\n",
       "1. Take the dot product of the query matrix Q with the transpose of the key matrix K^T. This gives a score showing the similarity between each query and key.\n",
       "\n",
       "2. Scale the scores by dividing by sqrt(d_k), where d_k is the dimension of the keys. This helps prevent the softmax function from being pushed into regions with extremely small gradients when d_k is large.\n",
       "\n",
       "3. Apply the softmax function to the scaled scores to obtain the attention weights.\n",
       "\n",
       "4. Multiply the attention weights with the value matrix V to get the weighted sum of the values, which are the attended outputs.\n",
       "\n",
       "This scaled dot-product attention mechanism allows efficiently computing attention by taking advantage of highly optimized matrix multiplication operations. It differs from additive attention by using the scaled dot-product similarity instead of a feed-forward network to compute attention scores."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the matched items\n",
    "matched_items = [items[index] for index in result.flatten()]\n",
    "\n",
    "# Generate a response using the RAG pipeline\n",
    "response = generate_rag_response(query, matched_items)\n",
    "\n",
    "# Display the response\n",
    "display.Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to Table 2 in the provided context, the Transformer (big) model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German (EN-DE) translation task."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the BLEU score of the model in English to German translation (EN-DE)?\"\n",
    "\n",
    "# Generate embeddings for the query\n",
    "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
    "\n",
    "# Search for the nearest neighbors in the vector database\n",
    "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)\n",
    "result.flatten()\n",
    "\n",
    "# Retrieve the matched items\n",
    "matched_items = [items[index] for index in result.flatten()]\n",
    "\n",
    "# Generate a response using the RAG pipeline\n",
    "response = generate_rag_response(query, matched_items)\n",
    "\n",
    "# Display the response\n",
    "display.Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "other_queries = [\"How long were the base and big models trained?\",\n",
    "                 \"Which optimizer was used when training the models?\",\n",
    "                 \"What is the position-wise feed-forward neural network mentioned in the paper?\",\n",
    "                 \"What is the BLEU score of the model in English to French translation (EN-FR)?\",\n",
    "                 \"What is the BLEU score of the model in English to German translation (EN-DE)?\",\n",
    "                 \"How is the scaled-dot-product attention is calculated?\",\n",
    "                 \"What is the BLEU score of the model in English to French translation (EN-FR)?\",\n",
    "                 \"What is the BLEU score of the model in English to German translation (EN-DE)?\",\n",
    "                 \"How is the scaled-dot-product attention is calculated?\",\n",
    "                 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the context, the base Transformer models were trained for a total of 100,000 steps or 12 hours, while the big Transformer models were trained for 300,000 steps or 3.5 days (on 8 P100 GPUs). Specifically, the context states:\n",
       "\n",
       "\"We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\"\n",
       "\n",
       "So the base models took 12 hours of training, while the bigger models took 3.5 days of training on 8 GPUs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = other_queries[0]\n",
    "\n",
    "# Generate embeddings for the query\n",
    "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
    "\n",
    "# Search for the nearest neighbors in the vector database\n",
    "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)\n",
    "result.flatten()\n",
    "\n",
    "# Retrieve the matched items\n",
    "matched_items = [items[index] for index in result.flatten()]\n",
    "\n",
    "# Generate a response using the RAG pipeline\n",
    "response = generate_rag_response(query, matched_items)\n",
    "\n",
    "# Display the response\n",
    "display.Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Learn More\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../img/github.png\" alt=\"Multimodal RAG with Amazon Bedrock\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Let's Connect\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../img/linkedin.png\" alt=\"LinkedIn\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Thank you!\n",
    "</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
